---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: ${parameter.projectName}-service
  annotations:
    openshift.io/display-name: AI sample application
    description: AI sample application
    tags: ai,service-delivery,${parameter.projectName}
labels:
  template: ${parameter.projectName}
parameters:
  # Resource Configuration
  - name: MEMORY_REQUEST
    description: Memory request for the API pods.
    value: ${parameters.memoryRequest}
  - name: MEMORY_LIMIT
    description: Memory limit for the API pods.
    value: ${parameters.memoryLimit}
  - name: CPU_REQUEST
    description: CPU request for the API pods.
    value: ${parameters.cpuRequest}
  - name: CPU_LIMIT
    description: CPU limit for the API pods.
    value: ${parameters.cpuLimit}
  # LLM Client Configuration
  - name: LLM_CLIENT_TYPE
    description: Type of LLM client to use (openai, langchain, llama_stack).
    value: ${parameters.llmClientType}
  - name: LANGCHAIN_PROVIDER
    description: LangChain provider to use (openai, ollama).
    value: ${parameters.langchainProvider}
  # Inference Configuration
  - name: INFERENCE_MODEL_NAME
    description: Model name to use for inference.
    value: ${parameters.inferenceModelName}
  - name: INFERENCE_BASE_URL
    description: Base URL for the inference API endpoint.
    value: ${parameters.inferenceBaseUrl}
  - name: INFERENCE_TEMPERATURE
    description: Temperature for response randomness (0.0-1.0).
    value: ${parameters.inferenceTemperature}
  - name: INFERENCE_MAX_TOKENS
    description: Maximum number of tokens to generate in response.
    value: ${parameters.inferenceMaxTokens}
  - name: SUMMARY_PROMPT
    description: System prompt for the AI assistant.
    value: ${parameters.summaryPrompt}

objects:
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: ${parameter.projectName}
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: ${parameter.projectName}
      template:
        metadata:
          labels:
            app: ${parameter.projectName}
        spec:
          containers:
            - name: ${parameter.projectName}
              image: quay.io/jbarea/hcm-ai-sample-app:1.0
              ports:
                - containerPort: 8000
              resources:
                requests:
                  memory: ${MEMORY_REQUEST}
                  cpu: ${CPU_REQUEST}
                limits:
                  memory: ${MEMORY_LIMIT}
                  cpu: ${CPU_LIMIT}
              env:
                # LLM Client Configuration
                - name: LLM_CLIENT_TYPE
                  value: ${LLM_CLIENT_TYPE}
                - name: LANGCHAIN_PROVIDER
                  value: ${LANGCHAIN_PROVIDER}
                
                # Inference Configuration
                - name: INFERENCE_API_KEY
                  valueFrom:
                    secretKeyRef:
                      name: ${parameter.projectName}-secret
                      key: INFERENCE_API_KEY
                - name: INFERENCE_MODEL_NAME
                  value: ${INFERENCE_MODEL_NAME}
                - name: INFERENCE_BASE_URL
                  value: ${INFERENCE_BASE_URL}
                
                # Optional Configuration (with defaults)
                - name: INFERENCE_TEMPERATURE
                  value: ${INFERENCE_TEMPERATURE}
                - name: INFERENCE_MAX_TOKENS
                  value: ${INFERENCE_MAX_TOKENS}
                - name: SUMMARY_PROMPT
                  value: ${SUMMARY_PROMPT}
              
              livenessProbe:
                httpGet:
                  path: /health
                  port: 8000
                initialDelaySeconds: 30
                periodSeconds: 10
                timeoutSeconds: 5
                failureThreshold: 3
              
              readinessProbe:
                httpGet:
                  path: /health
                  port: 8000
                initialDelaySeconds: 5
                periodSeconds: 5
                timeoutSeconds: 3
                failureThreshold: 2
              
              imagePullPolicy: Always
          securityContext:
            runAsNonRoot: true

  - apiVersion: v1
    kind: Service
    metadata:
      name: ${parameter.projectName}
    spec:
      selector:
        app: ${parameter.projectName}
      ports:
        - protocol: TCP
          port: 443
          targetPort: 8000

  - apiVersion: route.openshift.io/v1
    kind: Route
    metadata:
      name: ${parameter.projectName}
      annotations:
        haproxy.router.openshift.io/timeout: "1200s"
    spec:
      port:
        targetPort: 8000
      to:
        kind: Service
        name: ${parameter.projectName}
        weight: 100
      tls:
        termination: edge
      wildcardPolicy: None

  - apiVersion: v1
    kind: Secret
    metadata:
      name: ${parameter.projectName}-secret
    type: Opaque
    data:
      INFERENCE_API_KEY:  "{{ parameters.inferenceApiKey | base64 }}"
